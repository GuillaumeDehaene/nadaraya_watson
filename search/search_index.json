{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nadaraya-watson kernel regression","text":"<p>A simple implementation of the Nadaraya-Watson kernel regression estimator for usage with scikit-learn.</p> <p>Please note that the parameterization is slightly different from this other library. In my implementation, bandwidth is in units of distance, instead of being specific to the kernel.</p> <ul> <li>Github repository: https://github.com/GuillaumeDehaene/nadaraya_watson/</li> <li>Documentation https://GuillaumeDehaene.github.io/nadaraya_watson/</li> </ul>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding Nadaraya-Watson regression</li> <li>Installation</li> <li>Usage</li> <li>License</li> <li>Development</li> </ul>"},{"location":"#understanding-nadaraya-watson-regression","title":"Understanding Nadaraya-Watson regression","text":"<p>Nadaraya-Watson kernel regression is a non-parametric technique used for estimating the conditional expectation of a random variable. It works by placing a kernel function[^1] at each training data point and computing a weighted average of the target values, where weights are determined by the proximity of the query point to each training point.</p> <p>The prediction at a query point $ x $ is computed as:</p> \\[ \\hat{y}(x) = \\frac{\\sum_{i=1}^{n} K\\left(\\frac{d(x, x_i)}{h}\\right) y_i}{\\sum_{i=1}^{n} K\\left(\\frac{d(x, x_i)}{h}\\right)} \\] <p>where:</p> <ul> <li>\\(K(\\cdot)\\) is a kernel function[^1] (e.g., Gaussian, Epanechnikov)</li> <li>\\(h\\) is the bandwidth parameter, controlling the width of the kernel</li> <li>\\(d(x, x_i)\\) is the distance between the query point $ x $ and the training point $ x_i $</li> <li>\\(x_i\\) are the training points</li> <li>\\(y_i\\) are the corresponding target values</li> <li>\\(n\\) is the number of training samples</li> </ul> <p>In this implementation, the bandwidth $ h $ can be specified directly or computed using rules like Scott's or Silverman's rule. The kernel function used can be selected from a set of predefined kernels (e.g., Gaussian, Epanechnikov, etc.). The distance metric used to compute proximity between points is also configurable, defaulting to Euclidean distance.</p> <p>The method is flexible and robust, particularly useful when the underlying relationship between features and targets is unknown or complex.</p> <p>Please note that bounded kernels can produce <code>NaN</code>values. This is not a bug and is the expected behavior. If you are using a bounded kernel with a low bandwidth, then it is possible that some query points might intersect 0 training points. In this case, the only reasonable prediction is to return <code>NaN</code> to represent an impossible prediction.</p> <p>[^1]: in the context of Nadaraya-Watson regression, kernels refer to a positive function integrating to 1 and typically symmetric. It is important not to confuse this type of kernel, suitable for example in Kernel Density Estimation, with the kernels used in the \"Kernel Trick\" which respect instead Mercer's condition. Please refer to https://en.wikipedia.org/wiki/Kernel_(statistics) and https://en.wikipedia.org/wiki/Kernel_method for additional details.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install using <code>uv</code>, the extremely fast Python package and project manager, written in Rust.:</p> <pre><code>uv add git+https://github.com/GuillaumeDehaene/nadaraya_watson.git\n</code></pre> <p>Or using <code>pip</code>:</p> <pre><code>pip install git+https://github.com/GuillaumeDehaene/nadaraya_watson.git\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To use this estimator in your code, simply import and use it as you would any sklearn estimator.</p> <pre><code>from nadaraya_watson import KernelRegressionNW\n\nkernel_regression = KernelRegressionNW()\nkernel_regression.fit(x_fit, y_fit)\ny_pred = kernel_regression.predict(x_query)\n</code></pre> <p>My recommendation is to always use cross-validation, or some other hyperparameter selection scheme, to find the best value of the bandwidth, kernel, metric. Please refer to this example script.</p> <p>Please note that bounded kernels can produce <code>NaN</code>values. This is not a bug and is the expected behavior. If you are using a bounded kernel with a low bandwidth, then it is possible that some query points might intersect 0 training points. In this case, the only reasonable prediction is to return <code>NaN</code> to represent an impossible prediction. Please refer to this example script.</p>"},{"location":"#license","title":"License","text":"<p>Released under the MIT license.</p>"},{"location":"#development","title":"Development","text":"<p>This project is released as is with no guarantee of further development. I will only update it from time to time if I wish to play around with python tooling.</p> <p>Please do not create issues or reach out for updates. Feel free instead to adapt the code in any way you see fit while respecting the license.</p> <p>Repository initiated with fpgmaas/cookiecutter-uv.</p>"},{"location":"KernelRegressionNW/","title":"KernelRegressionNW","text":"<p>Nadaraya-Watson kernel regression is a non-parametric technique used for estimating the conditional expectation of a random variable. It works by placing a kernel function at each training data point and computing a weighted average of the target values, where weights are determined by the proximity of the query point to each training point.</p> <p>This class is a simple implementation of the Nadaraya-Watson kernel regression estimator for usage with scikit-learn.</p> <p>               Bases: <code>MultiOutputMixin</code>, <code>RegressorMixin</code>, <code>BaseEstimator</code></p> Source code in <code>src/nadaraya_watson/kernel_regression.py</code> <pre><code>class KernelRegressionNW(MultiOutputMixin, RegressorMixin, BaseEstimator):\n    # \"noqa: RUF012\"  : this is the sklearn default approach to parameter constraints\n    _parameter_constraints = {  # noqa: RUF012\n        \"bandwidth\": [\n            Interval(Real, 0, None, closed=\"neither\"),\n            StrOptions({\"scott\", \"silverman\"}),\n        ],\n        \"kernel\": [StrOptions(set(VALID_KERNELS))],\n        \"metric\": [StrOptions(set(VALID_METRICS))],\n    }\n\n    def __init__(\n        self,\n        *,\n        bandwidth: Union[float, Literal[\"scott\", \"silverman\"]] = 1.0,\n        kernel: str = \"gaussian\",\n        metric: str = \"euclidean\",\n    ):\n        \"\"\"Initialize the Nadaraya-Watson Kernel regression estimator.\n        Parameters\n        ----------\n        bandwidth : float or str, default=1.0\n            The bandwidth of the kernel. If a string, it must be one of \"scott\" or \"silverman\".\n        kernel : str, default=\"gaussian\"\n            The kernel to use. Must be one of the valid kernels.\n        metric : str, default=\"euclidean\"\n            The distance metric to use. Must be one of the valid metrics.\n        \"\"\"\n        self.bandwidth = bandwidth\n        self.kernel = kernel\n        self.metric = metric\n\n    @classmethod\n    def valid_metrics(cls) -&gt; list[str]:\n        \"\"\"Return a list of valid metrics.\n\n        Please note that some names are actually synonymous.\n        Please do not feed these values directly to sklearn for grid-search cross-validation.\n\n        e.g. \"euclidean\" and \"l2\" are identical.\n        Returns\n        -------\n        list of str\n            A list of valid metric names.\n        \"\"\"\n        return list(VALID_METRICS.keys())\n\n    @classmethod\n    def valid_kernels(cls) -&gt; list[str]:\n        \"\"\"Return a list of valid kernels.\n\n        Returns\n        -------\n        list of str\n            A list of valid kernel names.\n        \"\"\"\n        return list(VALID_KERNELS)\n\n    @_fit_context(\n        # KernelDensity.metric is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the Nadaraya-Watson Kernel regression estimator on the data.\n\n        Parameters\n        ----------\n        X : array-like\n            array of shape (n_samples, n_features)\n\n            Training data.\n        y : array-like\n            array of shape (n_samples,) or (n_samples, n_targets)\n\n            Target values.\n        sample_weight : array-like\n            array of shape (n_samples,)\n            Individual weights for each sample; ignored if None is passed.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        if isinstance(self.bandwidth, str):\n            if self.bandwidth == \"scott\":\n                self.bandwidth_ = X.shape[0] ** (-1 / (X.shape[1] + 4))\n            elif self.bandwidth == \"silverman\":\n                self.bandwidth_ = (X.shape[0] * (X.shape[1] + 2) / 4) ** (-1 / (X.shape[1] + 4))\n        else:\n            self.bandwidth_ = self.bandwidth\n        X, y = validate_data(self, X, y, accept_sparse=(\"csr\", \"csc\"), multi_output=True, y_numeric=True)\n        if sample_weight is not None and not isinstance(sample_weight, float):\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64, ensure_non_negative=True)\n\n        self.X_fit = X\n        self.y = y\n        self.sample_weight = sample_weight\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using Nadaraya-Watson Kernel regression estimator.\n\n        Parameters\n        ----------\n        X : array-like\n            array of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y_prediction : ndarray\n            array of shape (n_samples,) or (n_samples, n_targets)\n            Returns predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        X = validate_data(self, X, accept_sparse=(\"csr\", \"csc\"), reset=False)\n\n        # shape (n_samples_X_fit, n_samples_X_predict)\n        distance_matrix = pairwise_distances(self.X_fit, X, metric=self.metric)\n        log_density = KERNELS[self.kernel](distance_matrix / self.bandwidth_)\n\n        # Removing the max for numerical stability\n        # Removing a warning when subtracting -np.inf from a line of -np.inf : this produces a NaN and is expected behavior\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n            log_density -= np.max(log_density, axis=0, keepdims=True)\n\n        # Broadcast self.sample_weight to shape (n_samples_X_fit, n_samples_X_predict) from (n_samples_X_fit, )\n        weight = np.exp(log_density)\n        if self.sample_weight is not None:\n            weight *= self.sample_weight[:, None]\n        weight /= np.sum(weight, axis=0, keepdims=True)\n\n        # weighted sum of predictions\n        y_prediction = np.einsum(\"ij,ik-&gt;jk\", weight, self.y)\n\n        return y_prediction\n</code></pre>"},{"location":"KernelRegressionNW/#nadaraya_watson.KernelRegressionNW.__init__","title":"<code>__init__(*, bandwidth=1.0, kernel='gaussian', metric='euclidean')</code>","text":"<p>Initialize the Nadaraya-Watson Kernel regression estimator.</p> <p>Parameters:</p> Name Type Description Default <code>bandwidth</code> <code>float or str</code> <p>The bandwidth of the kernel. If a string, it must be one of \"scott\" or \"silverman\".</p> <code>1.0</code> <code>kernel</code> <code>str</code> <p>The kernel to use. Must be one of the valid kernels.</p> <code>\"gaussian\"</code> <code>metric</code> <code>str</code> <p>The distance metric to use. Must be one of the valid metrics.</p> <code>\"euclidean\"</code> Source code in <code>src/nadaraya_watson/kernel_regression.py</code> <pre><code>def __init__(\n    self,\n    *,\n    bandwidth: Union[float, Literal[\"scott\", \"silverman\"]] = 1.0,\n    kernel: str = \"gaussian\",\n    metric: str = \"euclidean\",\n):\n    \"\"\"Initialize the Nadaraya-Watson Kernel regression estimator.\n    Parameters\n    ----------\n    bandwidth : float or str, default=1.0\n        The bandwidth of the kernel. If a string, it must be one of \"scott\" or \"silverman\".\n    kernel : str, default=\"gaussian\"\n        The kernel to use. Must be one of the valid kernels.\n    metric : str, default=\"euclidean\"\n        The distance metric to use. Must be one of the valid metrics.\n    \"\"\"\n    self.bandwidth = bandwidth\n    self.kernel = kernel\n    self.metric = metric\n</code></pre>"},{"location":"KernelRegressionNW/#nadaraya_watson.KernelRegressionNW.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the Nadaraya-Watson Kernel regression estimator on the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>array of shape (n_samples, n_features)</p> <p>Training data.</p> required <code>y</code> <code>array - like</code> <p>array of shape (n_samples,) or (n_samples, n_targets)</p> <p>Target values.</p> required <code>sample_weight</code> <code>array - like</code> <p>array of shape (n_samples,) Individual weights for each sample; ignored if None is passed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns the instance itself.</p> Source code in <code>src/nadaraya_watson/kernel_regression.py</code> <pre><code>@_fit_context(\n    # KernelDensity.metric is not validated yet\n    prefer_skip_nested_validation=False\n)\ndef fit(self, X, y, sample_weight=None):\n    \"\"\"Fit the Nadaraya-Watson Kernel regression estimator on the data.\n\n    Parameters\n    ----------\n    X : array-like\n        array of shape (n_samples, n_features)\n\n        Training data.\n    y : array-like\n        array of shape (n_samples,) or (n_samples, n_targets)\n\n        Target values.\n    sample_weight : array-like\n        array of shape (n_samples,)\n        Individual weights for each sample; ignored if None is passed.\n\n    Returns\n    -------\n    self : object\n        Returns the instance itself.\n    \"\"\"\n    if isinstance(self.bandwidth, str):\n        if self.bandwidth == \"scott\":\n            self.bandwidth_ = X.shape[0] ** (-1 / (X.shape[1] + 4))\n        elif self.bandwidth == \"silverman\":\n            self.bandwidth_ = (X.shape[0] * (X.shape[1] + 2) / 4) ** (-1 / (X.shape[1] + 4))\n    else:\n        self.bandwidth_ = self.bandwidth\n    X, y = validate_data(self, X, y, accept_sparse=(\"csr\", \"csc\"), multi_output=True, y_numeric=True)\n    if sample_weight is not None and not isinstance(sample_weight, float):\n        sample_weight = _check_sample_weight(sample_weight, X)\n\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64, ensure_non_negative=True)\n\n    self.X_fit = X\n    self.y = y\n    self.sample_weight = sample_weight\n\n    return self\n</code></pre>"},{"location":"KernelRegressionNW/#nadaraya_watson.KernelRegressionNW.predict","title":"<code>predict(X)</code>","text":"<p>Predict using Nadaraya-Watson Kernel regression estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>array of shape (n_samples, n_features) Samples.</p> required <p>Returns:</p> Name Type Description <code>y_prediction</code> <code>ndarray</code> <p>array of shape (n_samples,) or (n_samples, n_targets) Returns predicted values.</p> Source code in <code>src/nadaraya_watson/kernel_regression.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict using Nadaraya-Watson Kernel regression estimator.\n\n    Parameters\n    ----------\n    X : array-like\n        array of shape (n_samples, n_features)\n        Samples.\n\n    Returns\n    -------\n    y_prediction : ndarray\n        array of shape (n_samples,) or (n_samples, n_targets)\n        Returns predicted values.\n    \"\"\"\n    check_is_fitted(self)\n    X = validate_data(self, X, accept_sparse=(\"csr\", \"csc\"), reset=False)\n\n    # shape (n_samples_X_fit, n_samples_X_predict)\n    distance_matrix = pairwise_distances(self.X_fit, X, metric=self.metric)\n    log_density = KERNELS[self.kernel](distance_matrix / self.bandwidth_)\n\n    # Removing the max for numerical stability\n    # Removing a warning when subtracting -np.inf from a line of -np.inf : this produces a NaN and is expected behavior\n    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n        log_density -= np.max(log_density, axis=0, keepdims=True)\n\n    # Broadcast self.sample_weight to shape (n_samples_X_fit, n_samples_X_predict) from (n_samples_X_fit, )\n    weight = np.exp(log_density)\n    if self.sample_weight is not None:\n        weight *= self.sample_weight[:, None]\n    weight /= np.sum(weight, axis=0, keepdims=True)\n\n    # weighted sum of predictions\n    y_prediction = np.einsum(\"ij,ik-&gt;jk\", weight, self.y)\n\n    return y_prediction\n</code></pre>"},{"location":"KernelRegressionNW/#nadaraya_watson.KernelRegressionNW.valid_kernels","title":"<code>valid_kernels()</code>  <code>classmethod</code>","text":"<p>Return a list of valid kernels.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>A list of valid kernel names.</p> Source code in <code>src/nadaraya_watson/kernel_regression.py</code> <pre><code>@classmethod\ndef valid_kernels(cls) -&gt; list[str]:\n    \"\"\"Return a list of valid kernels.\n\n    Returns\n    -------\n    list of str\n        A list of valid kernel names.\n    \"\"\"\n    return list(VALID_KERNELS)\n</code></pre>"},{"location":"KernelRegressionNW/#nadaraya_watson.KernelRegressionNW.valid_metrics","title":"<code>valid_metrics()</code>  <code>classmethod</code>","text":"<p>Return a list of valid metrics.</p> <p>Please note that some names are actually synonymous. Please do not feed these values directly to sklearn for grid-search cross-validation.</p> <p>e.g. \"euclidean\" and \"l2\" are identical.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>A list of valid metric names.</p> Source code in <code>src/nadaraya_watson/kernel_regression.py</code> <pre><code>@classmethod\ndef valid_metrics(cls) -&gt; list[str]:\n    \"\"\"Return a list of valid metrics.\n\n    Please note that some names are actually synonymous.\n    Please do not feed these values directly to sklearn for grid-search cross-validation.\n\n    e.g. \"euclidean\" and \"l2\" are identical.\n    Returns\n    -------\n    list of str\n        A list of valid metric names.\n    \"\"\"\n    return list(VALID_METRICS.keys())\n</code></pre>"},{"location":"kernels/","title":"kernels","text":"<p>Since the built-in log-kernels are internal to sklearn, I had to provide my own implementation.</p>"},{"location":"kernels/#nadaraya_watson.kernels.log_cosine_kernel","title":"<code>log_cosine_kernel(dist)</code>","text":"<p>log of the cosine kernel (unnormalized)</p> Source code in <code>src/nadaraya_watson/kernels.py</code> <pre><code>def log_cosine_kernel(dist: np.ndarray) -&gt; np.ndarray:\n    \"\"\"log of the cosine kernel (unnormalized)\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log\")\n        warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in log\")\n        return np.where(dist &lt; 1.0, np.log(np.cos(0.5 * np.pi * dist)), -np.inf)\n</code></pre>"},{"location":"kernels/#nadaraya_watson.kernels.log_epanechnikov_kernel","title":"<code>log_epanechnikov_kernel(dist)</code>","text":"<p>log of the epanechnikov kernel (unnormalized)</p> Source code in <code>src/nadaraya_watson/kernels.py</code> <pre><code>def log_epanechnikov_kernel(dist: np.ndarray) -&gt; np.ndarray:\n    \"\"\"log of the epanechnikov kernel (unnormalized)\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log\")\n        warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in log\")\n        return np.where(dist &lt; 1.0, np.log(1.0 - (dist * dist)), -np.inf)\n</code></pre>"},{"location":"kernels/#nadaraya_watson.kernels.log_exponential_kernel","title":"<code>log_exponential_kernel(dist)</code>","text":"<p>log of the exponential kernel (unnormalized)</p> Source code in <code>src/nadaraya_watson/kernels.py</code> <pre><code>def log_exponential_kernel(dist: np.ndarray) -&gt; np.ndarray:\n    \"\"\"log of the exponential kernel (unnormalized)\"\"\"\n    return -np.abs(dist)\n</code></pre>"},{"location":"kernels/#nadaraya_watson.kernels.log_gaussian_kernel","title":"<code>log_gaussian_kernel(dist)</code>","text":"<p>log of the gaussian kernel (unnormalized)</p> Source code in <code>src/nadaraya_watson/kernels.py</code> <pre><code>def log_gaussian_kernel(dist: np.ndarray) -&gt; np.ndarray:\n    \"\"\"log of the gaussian kernel (unnormalized)\"\"\"\n    return -0.5 * (dist * dist)\n</code></pre>"},{"location":"kernels/#nadaraya_watson.kernels.log_triangular_kernel","title":"<code>log_triangular_kernel(dist)</code>","text":"<p>log of the triangular kernel (unnormalized)</p> Source code in <code>src/nadaraya_watson/kernels.py</code> <pre><code>def log_triangular_kernel(dist: np.ndarray) -&gt; np.ndarray:\n    \"\"\"log of the triangular kernel (unnormalized)\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log\")\n        warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in log\")\n        return np.where(dist &lt; 1.0, np.log(1 - dist), -np.inf)\n</code></pre>"},{"location":"kernels/#nadaraya_watson.kernels.log_uniform_kernel","title":"<code>log_uniform_kernel(dist)</code>","text":"<p>log of the uniform kernel (unnormalized)</p> Source code in <code>src/nadaraya_watson/kernels.py</code> <pre><code>def log_uniform_kernel(dist: np.ndarray) -&gt; np.ndarray:\n    \"\"\"log of the uniform kernel (unnormalized)\"\"\"\n    return np.where(dist &lt; 1.0, 0.0, -np.inf).astype(dist.dtype)\n</code></pre>"}]}